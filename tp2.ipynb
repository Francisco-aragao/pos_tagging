{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Francisco Teixeira Rocha Arag√£o 2021031726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Set logging level\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /neuralmind/bert-base-portuguese-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /neuralmind/bert-base-portuguese-cased/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "# Create a device object\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Set max_length\n",
    "max_length = 512\n",
    "\n",
    "def preprocess_and_tokenize(data):\n",
    "    \"\"\"Preprocesses and tokenizes the input data.\n",
    "    Returns tokenized inputs, padded POS tags, and attention masks. The\n",
    "    input_ids and padded_pos_tags are padded to `max_length`. There is a\n",
    "    one-to-one correspondence between the tokens, POS tags and\n",
    "    attention masks.\\\n",
    "    Arguments:\n",
    "        data: list of strings. Each string is a sentence with words and POS\n",
    "            tags separated by an underscore. For example,\n",
    "            \"Jersei_N atinge_V m√©dia_N de_PREP\".\n",
    "    Returns:\n",
    "        input_ids: torch.tensor of shape (num_sentences, max_length).\n",
    "            It is a \"list\" of numerical values (ids) that represent each token\n",
    "            in the input text. The values are based on the vocabulary of the\n",
    "            pre-trained BERT model.\n",
    "        padded_pos_tags: torch.tensor of shape (num_sentences, max_length).\n",
    "            It is a \"list\" of numerical values (ids) that represent each POS\n",
    "            tag in the input text. The values are based on the label encoder.\n",
    "        attention_masks: torch.tensor of shape (num_sentences, max_length).\n",
    "            It is a \"list\" of 0s and 1s. The 1s indicate the position of the\n",
    "            tokens in the input_ids tensor. The 0s indicate the padding.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    pos_tags = []\n",
    "    unique_tags = set()\n",
    "\n",
    "    # Split data into sentences and POS tags\n",
    "    # count = 0\n",
    "    for line in tqdm(data, desc=\"Splitting data into sentences and POS tags\"):\n",
    "        words = []\n",
    "        tags = []\n",
    "        #logging.debug(f\"line: {line}\")\n",
    "        for word in line.split():\n",
    "            split_word = word.split('_')\n",
    "            #logging.debug(f\"split_word: {split_word}\")\n",
    "            words.append(split_word[0])\n",
    "            tags.append(split_word[1])\n",
    "            unique_tags.add(split_word[1])\n",
    "            #logging.debug(f\"words: {words}\")\n",
    "            #logging.debug(f\"unique_tags: {unique_tags}\")\n",
    "        sentences.append(words)\n",
    "        pos_tags.append(tags)\n",
    "        # count += 1\n",
    "        # if count > 50:\n",
    "        #     break\n",
    "\n",
    "    # Count the number of sentences and tags\n",
    "    num_sentences = sum(len(sentence) for sentence in sentences)\n",
    "    num_tags = sum(len(tags) for tags in pos_tags)\n",
    "    print(f\"Number of sentences: {num_sentences}\")\n",
    "    #logging.debug(f\"Number of tags (classes): {num_tags}\")\n",
    "    print(f\"Number of unique tags: {len(unique_tags)}\")\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokenized_inputs = bert_tokenizer(sentences, truncation=True, padding='max_length',\n",
    "                                      max_length=max_length, is_split_into_words=True)\n",
    "\n",
    "    # Pad input_ids to max_length\n",
    "    input_ids = pad_sequence([torch.tensor(i) for i in tokenized_inputs[\"input_ids\"]], batch_first=True)\n",
    "    input_ids = input_ids.to(device)  # Move to GPU\n",
    "    # FIXME: Do this next to avoid unnecessary conversions\n",
    "    # input_ids = pad_sequence(tokenized_inputs[\"input_ids\"], batch_first=True).to(device)\n",
    "    #logging.debug(f\"input_ids: {input_ids}\")\n",
    "\n",
    "    # Handle the POS tags\n",
    "    new_pos_tags = []\n",
    "    for sent_tags, input_id in zip(pos_tags, tokenized_inputs[\"input_ids\"]):\n",
    "        new_tags = []\n",
    "        for tag in sent_tags:\n",
    "            new_tags.extend([tag] * len(input_id))\n",
    "        new_pos_tags.append(new_tags[:len(input_id)])\n",
    "\n",
    "    # Fit the label encoder with the unique tags\n",
    "    label_encoder.fit(list(unique_tags))\n",
    "    # Encode the POS tags\n",
    "    # encoded_pos_tags = [label_encoder.fit_transform(tags) for tags in new_pos_tags]\n",
    "    encoded_pos_tags = [label_encoder.transform(tags) for tags in pos_tags]\n",
    "    #logging.debug(f\"encoded_pos_tags: {encoded_pos_tags}\")\n",
    "\n",
    "    # Pad the POS tags\n",
    "    # padded_pos_tags = pad_sequence([torch.tensor(tags) for tags in encoded_pos_tags], batch_first=True)\n",
    "    # padded_pos_tags = padded_pos_tags.to(device)  # Move to GPU\n",
    "    padded_pos_tags = [F.pad(torch.tensor(tags), pad=(0, max_length - len(tags))) for tags in encoded_pos_tags]\n",
    "    padded_pos_tags = torch.stack(padded_pos_tags).to(device)  # Stack the list of tensors into a single tensor and move to GPU\n",
    "    #logging.debug(f\"padded_pos_tags: {padded_pos_tags}\")\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = [[float(i != 0.0) for i in seq] for seq in input_ids]\n",
    "    attention_masks = torch.tensor(attention_masks).to(device)  # Move to GPU\n",
    "    #logging.debug(f\"attention_masks: {attention_masks}\")\n",
    "\n",
    "    print(f\"Number of sequences (dataset lines): {input_ids.shape[0]}\")\n",
    "    print()\n",
    "\n",
    "    return input_ids, padded_pos_tags, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data into sentences and POS tags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37948/37948 [00:00<00:00, 194880.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 728497\n",
      "Number of unique tags: 26\n",
      "Number of sequences (dataset lines): 37948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting data into sentences and POS tags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1997/1997 [00:00<00:00, 210389.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 38881\n",
      "Number of unique tags: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences (dataset lines): 1997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('data/macmorpho-train.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    input_ids, tags, masks = preprocess_and_tokenize(data)\n",
    "\n",
    "with open('data/macmorpho-dev.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    val_input_ids, val_tags, val_masks = preprocess_and_tokenize(data)\n",
    "\n",
    "UNIQUE_TAGS = 26\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "https://huggingface.co:443 \"HEAD /bert-base-multilingual-cased/resolve/main/model.safetensors HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 139866583868016 on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "Attempting to acquire lock 139866583868016 on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "DEBUG:filelock:Lock 139866583868016 acquired on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "Lock 139866583868016 acquired on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs.hf.co:443\n",
      "Starting new HTTPS connection (1): cdn-lfs.hf.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs.hf.co:443 \"GET /bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737071961&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzA3MTk2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9iZXJ0LWJhc2UtbXVsdGlsaW5ndWFsLWNhc2VkLzg3NmY1ODRmMTVlYmYxNDg4N2RlYzE3NTM5YzExNGJiOTlhMDMyZTk2YjllNzI1MDdhNTFjNDFlMjA1MzM3ZmM~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=f11jHT17b2QQ6X-KNxsgwoIBLuxJsKLXsAylYbdIie9kssauGw47vAb2UCCr7p5uPVGz1hR~UlW0ITji0fEox2EprFRPWh1W2CPtMlUapyCObolJYB8St5-0FtC9bosCoXRvnus1YnYO5u8gOqQclhDjb7cRDdg92y2YCMXB1XjmeXqxIu5UWwKdtocM2wLkQVhOi3vYa85N0h2v9HNBB31h64BNFrR7FUaQQpiB8JgrlqLJ8Uibjm7EK-5I5G~yryiMkPn-OKqbCmyDMPdrgDrRmcRdI32uD2~z5GDpNydkbjQNGLWhAa-AfiFrtl3lPKmrwP5-4e6xKjn7KDm~pA__&Key-Pair-Id=K3RPWS32NSSJCE HTTP/1.1\" 200 714290682\n",
      "https://cdn-lfs.hf.co:443 \"GET /bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1737071961&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczNzA3MTk2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9iZXJ0LWJhc2UtbXVsdGlsaW5ndWFsLWNhc2VkLzg3NmY1ODRmMTVlYmYxNDg4N2RlYzE3NTM5YzExNGJiOTlhMDMyZTk2YjllNzI1MDdhNTFjNDFlMjA1MzM3ZmM~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=f11jHT17b2QQ6X-KNxsgwoIBLuxJsKLXsAylYbdIie9kssauGw47vAb2UCCr7p5uPVGz1hR~UlW0ITji0fEox2EprFRPWh1W2CPtMlUapyCObolJYB8St5-0FtC9bosCoXRvnus1YnYO5u8gOqQclhDjb7cRDdg92y2YCMXB1XjmeXqxIu5UWwKdtocM2wLkQVhOi3vYa85N0h2v9HNBB31h64BNFrR7FUaQQpiB8JgrlqLJ8Uibjm7EK-5I5G~yryiMkPn-OKqbCmyDMPdrgDrRmcRdI32uD2~z5GDpNydkbjQNGLWhAa-AfiFrtl3lPKmrwP5-4e6xKjn7KDm~pA__&Key-Pair-Id=K3RPWS32NSSJCE HTTP/1.1\" 200 714290682\n",
      "DEBUG:filelock:Attempting to release lock 139866583868016 on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "Attempting to release lock 139866583868016 on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "DEBUG:filelock:Lock 139866583868016 released on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "Lock 139866583868016 released on /home/francisco/.cache/huggingface/hub/.locks/models--bert-base-multilingual-cased/876f584f15ebf14887dec17539c114bb99a032e96b9e72507a51c41e205337fc.lock\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/francisco/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a custom PyTorch Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, masks, tags):\n",
    "        self.input_ids = input_ids\n",
    "        self.masks = masks\n",
    "        self.tags = tags.to('cpu')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.masks[idx],\n",
    "            'labels': self.tags[idx]\n",
    "        }\n",
    "\n",
    "# Move the tensors to the CPU\n",
    "input_ids_cpu = input_ids.to('cpu')\n",
    "masks_cpu = masks.to('cpu')\n",
    "tags_cpu = tags.to('cpu')\n",
    "val_input_ids_cpu = val_input_ids.to('cpu')\n",
    "val_masks_cpu = val_masks.to('cpu')\n",
    "val_tags_cpu = val_tags.to('cpu')\n",
    "\n",
    "# Convert training data into PyTorch Dataset\n",
    "# train_dataset = CustomDataset(input_ids, masks, tags)\n",
    "train_dataset = CustomDataset(input_ids_cpu, masks_cpu, tags_cpu)\n",
    "\n",
    "# Convert validation data into PyTorch Dataset\n",
    "# eval_dataset = CustomDataset(val_input_ids, val_masks, val_tags)\n",
    "eval_dataset = CustomDataset(val_input_ids_cpu, val_masks_cpu, val_tags_cpu)\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',\n",
    "    num_labels=UNIQUE_TAGS,\n",
    ")\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16, # 4, 16\n",
    "    per_device_eval_batch_size=32,  # 4, 32, 64\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=1000,  # Set logging_steps to see the validation loss and metrics\n",
    "    # bf16=True, # or fp16, to reduce memory/computer usage/requirements\n",
    "    save_strategy=\"epoch\",\n",
    "    dataloader_num_workers=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    labels = np.ravel(eval_pred.label_ids)\n",
    "    preds = np.ravel(eval_pred.predictions.argmax(-1))\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_layer: Linear(in_features=768, out_features=26, bias=True)\n"
     ]
    }
   ],
   "source": [
    "classification_layer = model.classifier\n",
    "print(f\"classification_layer: {classification_layer}\")\n",
    "\n",
    "# Freeze all the layers of the BERT model\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Train only the classifier layer\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)\n",
    "\n",
    "# Check that only the classifier layer is trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:  # Check if the parameter belongs to the classifier layer\n",
    "        assert param.requires_grad == True  # Assert that the classifier layer parameters are trainable\n",
    "    else:\n",
    "        assert param.requires_grad == False  # Assert that all other parameters are not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='7116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/7116 02:50 < 14:01:49, 0.14 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the training loss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_output\u001b[38;5;241m.\u001b[39mtraining_loss)\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/trainer.py:3678\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3676\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3677\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3678\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3682\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3683\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3684\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/trainer.py:3734\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3732\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3733\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3734\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1859\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1859\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1873\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1108\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[1;32m   1102\u001b[0m             attention_mask,\n\u001b[1;32m   1103\u001b[0m             input_shape,\n\u001b[1;32m   1104\u001b[0m             embedding_output,\n\u001b[1;32m   1105\u001b[0m             past_key_values_length,\n\u001b[1;32m   1106\u001b[0m         )\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1108\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:451\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:192\u001b[0m, in \u001b[0;36mAttentionMaskConverter._expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    188\u001b[0m expanded_mask \u001b[38;5;241m=\u001b[39m mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mexpand(bsz, \u001b[38;5;241m1\u001b[39m, tgt_len, src_len)\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[1;32m    190\u001b[0m inverted_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m expanded_mask\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minverted_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverted_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_output = trainer.train()\n",
    "\n",
    "# Print the training loss\n",
    "print(train_output.training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/macmorpho-test.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    test_input_ids, test_tags, test_masks = preprocess_and_tokenize(data)\n",
    "\n",
    "test_input_ids_cpu = test_input_ids.to('cpu')\n",
    "test_masks_cpu = test_masks.to('cpu')\n",
    "test_tags_cpu = test_tags.to('cpu')\n",
    "\n",
    "# Convert test data into PyTorch Dataset\n",
    "test_dataset = CustomDataset(test_input_ids_cpu, test_masks_cpu, test_tags_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "trainer.args.per_device_eval_batch_size = 4  # reduce evaluation batch size\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Make predictions\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "     \n",
    "\n",
    "def get_words_from_input_ids(input_ids):\n",
    "    words = []\n",
    "    for ids in input_ids:\n",
    "        words.append(bert_tokenizer.convert_ids_to_tokens(ids))\n",
    "    return words\n",
    "\n",
    "# Obt√©m as palavras a partir dos tokens\n",
    "words = get_words_from_input_ids(test_input_ids)\n",
    "\n",
    "# Apply softmax to get probabilities and then get the most probable tags\n",
    "predictions = np.argmax(F.softmax(torch.from_numpy(predictions), dim=-1), axis=-1)\n",
    "\n",
    "# Flatten predictions and labels\n",
    "predictions = predictions.flatten()\n",
    "labels = labels.flatten()\n",
    "\n",
    "# Create a DataFrame with columns 'true', 'predicted', 'word'\n",
    "df = pd.DataFrame({\n",
    "    'true': [tags[val] for val in labels],\n",
    "    'predicted': [tags[val] for val in predictions],\n",
    "    'word': [words[val] for val in input_ids_cpu.flatten()]\n",
    "})\n",
    "\n",
    "# Find the words where the true label is equal to the predicted label\n",
    "correct_predictions = df[df['true'] == df['predicted']]\n",
    "\n",
    "# Find the words where the true label is not equal to the predicted label\n",
    "incorrect_predictions = df[df['true'] != df['predicted']]\n",
    "\n",
    "print(\"## Palavras que tiveram a classe gramatical corretamente acertadas\")\n",
    "print(correct_predictions['word'].value_counts().head())\n",
    "\n",
    "print()\n",
    "print(\"## Palavras onde a predi√ß√£o da classe gramatical foi errada, por pelo menos uma vez\")\n",
    "print(incorrect_predictions['word'].value_counts().head())\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
