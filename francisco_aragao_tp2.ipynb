{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Francisco Teixeira Rocha Aragão 2021031726\n",
    "## Processamento de Língua Natural - Trabalho Prático 2\n",
    "\n",
    "### Tarefa\n",
    "\n",
    "Nesse arquivo contem a implementação da solução para um problema de 'pos tagging', que é a tarefa de atribuir uma tag a cada palavra de uma sentença, em que no caso deste trabalho, envolve a atribuição de tags de classes gramáticais para frases em português.\n",
    "\n",
    "Os dados utilizados são do corpus MacMorpho, que contém textos em português com as respectivas tags de classes gramaticais. O corpus já está dividido em 3 partes: treino, validação e teste. É possível baixar o corpus no [link referenciado](http://nilc.icmc.usp.br/macmorpho/macmorpho-v3.tgz)\n",
    "\n",
    "### Metodologia\n",
    "\n",
    "Desse modo, o trabalho desenvolvido envolve a utilização de modelos para realizar a tarefa de pos tagging, com o desempenho sendo medido e retornando em cada caso testado. Vale destacar inicialmente que algumas estratégias foram implementadas, porém não foram bem sucedidas, como o treinamento de muitas camadas do modelo BERT em português, ou a utilização de LLMs para a tarefa de pos tagging. A falta de sucesso deve-se a necessidade de maior poder de processamento, GPU e de memória para trabalhar com tarefas tão complexas e modelos tão grandes, o que não foi possível de ser feito localmente.\n",
    "\n",
    "Com isso, a estratégia adotada foi de utilizar modelos já especializados na tarefa de pós taggins (modelos que sofreram fine-tuning). Assim, os modelos escolhidos foram treinandos utilizando o próprio dataset MacMorpho, além de terem como base modelos BERT treinados em língua portuguesa. Fazer o uso desses modelos foi de grande ajuda pois não foi necessário o treinamento, apenas carregar em memória e utilizar.\n",
    "\n",
    "Link para os modelos utilizados:\n",
    "\n",
    "- [Modelo 1](https://huggingface.co/pucpr-br/postagger-bio-portuguese)\n",
    "\n",
    "    - [Modelo base para o modelo 1](https://huggingface.co/pucpr/biobertpt-all)\n",
    "\n",
    "- [Modelo 2](https://huggingface.co/lisaterumi/postagger-portuguese)\n",
    "\n",
    "    - [Modelo base para o modelo 2](https://github.com/neuralmind-ai/portuguese-bert/)\n",
    "\n",
    "### Resultados\n",
    "\n",
    "Os resultados númericos dos modelos estão presentes no decorrer do código, porém são comentados a seguir:\n",
    "\n",
    "### Análise dos Resultados\n",
    "\n",
    "Percebe-se que no geral, o primeiro modelo conseguiu uma boa performance, com uma acurácia geral de 68%. Isso mostra que o modelo conseguiu uma boa quantidade de acertos. No entanto, como temos várias tags diferentes para as palavras, é importante observar o resultado de maneira mais aprofundada. Ao Analisar o resultado de acurácia por tag, percebe-se tendências diferentes do que foi visto ao analisar o resultado geral. Primeiramente, as tags que possuem muitas ocorrências, como N (Nome), KC (Conjunção Coordenativa) e PU (Pontuação) que contam com mais de 70 mil ocorrências, foram algumas das tags com melhores resultados, obtendo uma acurácia maior do que 94%. O mesmo resultado pode ser observado para outras tags que estão listadas no decorrer do código. Vale observar que a média geral caiu por conta de tags mais específicas, como PREP+PRO-KS (Preposição + Pronome Conectivo Subordinativo), ADV-KS (Advérbio Conectivo Subordinativo) e CUR (Currency - medida monetária) que obtiveram acurácia de 0%. Por serem palavras mais específicas, obtendo menos de 2 mil ocorrências (PREP+PRO-KS obteve menos de 100 ocorrências), tal resultado era esperado visto que aparecem menos nos dados de treino (e até mesmo no nosso vocabulário cotidiano).\n",
    "Também foram medidos precisão, recall e f1 de diferentes tags. Tags como KC e PU obtiveram altos resultados (mais de 90%), tanto de precisão quanto de recall (consequentemente f1 também). No entanto, N (nomes) obteve um resultado ruim de precisão (67%) quando comparado ao recall (96%). Isso mostra que muitas palavras foram classificadas como N, porém não eram. O que também é um possível resultado esperado visto que N é a tag que mais ocorre no dataset, podendo ser confundida com outras ao fazer a tarefa de classificação. Já NPROP (Nome próprio) obteve resultado contrários, com mais de 90% de precisão e 50% de recall. Isso mostra que muitos casos de NPROP foram classificados com outras tags. O que é interessante de se ver, visto que NPROP obteve quase metade das ocorrências de N.\n",
    "\n",
    "Já pro segundo modelo, vemos um resultado semelhante. Este obteve uma acurácia geral maior do que o primeiro, com 74%. Ao observar os resultados específicos, temos um cenário semelhante, com tags como PU, N, KC possuindo uma acurácia bem alta novamente. Vale destacar inclusive que PU obteve 100% de acurácia. De modo geral o cenário visto anteriormente se repete, mesmo que tenham diferenças em tags específicas, como PROSUB (Pronome Substantivo) que antes obteve menos de 60% de acurácia e agora obteve mais de 85%. PROADJ (Pronome Adjetivo) que antes obteve uma acurácia média (43%), agora obteve 5%, sendo um resultado bem diferente em comparação a ambos os modelos.\n",
    "Falando agora sobre precisão recall e f1, o cenário geral novamente mantêm-se similar, com as melhores / piores tags classificadas sendo semelhantes. Alguns casos tiveram variações, como PU que antes obteve 95% de F1, com um bom balanço de precisão e recall, porém agora obteve 82%, motivado pela precisão mais baixa (70%). Outras tendências gerais se manteram, como ADJ que antes obteve alta precisão (92%) e baixo recall (36%) e agora, mesmo com melhor f1 (antes 52% agora 64%), manteve essa diferença com alta precisão (89%) e recall mais baixo (50%).\n",
    "\n",
    "Portanto, percebe-se nesse trabalho a dificuldade em trabalhar com grandes modelos, como BERT e LLMs, sem o poder de processamento necessário, tanto para treinamento quanto para uso. No entanto, a utilização de modelos já treinados e especializados na tarefa de pos tagging mostrou-se eficiente, com resultados satisfatórios. Ambos os modelos utilizados que possuiram treinos semelhantes, obtiveram também resultados semelhantes, embora com algumas diferenças em tags específicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francisco/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import das bibliotecas utilizadas\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções úteis para cálculo de precisão, acurácia, recall e f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_and_print_general_accuracy(results, total_words):\n",
    "    correct_predictions = 0\n",
    "    for result in results:\n",
    "        if result['correct']:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_words\n",
    "    print(f'Acurácia geral: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_and_print_accuracy_and_len_by_tag(results):\n",
    "    correct_tags = {}\n",
    "    total_tags = {}\n",
    "\n",
    "    for result in results:\n",
    "        if result['tag'] not in total_tags:\n",
    "            total_tags[result['tag']] = 1\n",
    "        else:\n",
    "            total_tags[result['tag']] += 1\n",
    "        \n",
    "        if result['correct']:\n",
    "            if result['tag'] not in correct_tags:\n",
    "                correct_tags[result['tag']] = 1\n",
    "            else:\n",
    "                correct_tags[result['tag']] += 1\n",
    "\n",
    "    accuracy_tags = {}\n",
    "    for tag in total_tags:\n",
    "        try:\n",
    "            accuracy_tags[tag] = correct_tags[tag] / total_tags[tag]\n",
    "        except KeyError:\n",
    "            accuracy_tags[tag] = 0\n",
    "\n",
    "    couting_tags = {}\n",
    "    for tag, total in total_tags.items():\n",
    "        couting_tags[tag] = total\n",
    "\n",
    "    # print dos resultados em ordem decrescente de acurácia\n",
    "    for tag, accuracy in sorted(accuracy_tags.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f'TAG: {tag} - ACURÁCIA: {accuracy:.2f} - OCORRÊNCIAS: {couting_tags[tag]}')\n",
    "    \n",
    "    return total_tags, accuracy_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_and_print_precision_recall_f1_by_tag(results):\n",
    "    \n",
    "    # contando TRUE_POSITIVES, FALSE_POSITIVES e FALSE_NEGATIVES para cada tag\n",
    "    # FALSE_POSITIVE: previu como essa tag, mas não era\n",
    "    # FALSE_NEGATIVE: era essa tag, mas previu como outra\n",
    "    # TRUE_POSITIVE: previu como essa tag e era essa tag\n",
    "    true_positives = defaultdict(int)\n",
    "    false_positives = defaultdict(int)\n",
    "    false_negatives = defaultdict(int)\n",
    "\n",
    "    for result in results:\n",
    "        predicted_tag = result['prediction']\n",
    "        expected_tag = result['tag']\n",
    "        correct_prediction = result[\"correct\"]\n",
    "        \n",
    "        if correct_prediction:  # True Positive\n",
    "            true_positives[predicted_tag] += 1\n",
    "        else:\n",
    "            # False Positive\n",
    "            false_positives[predicted_tag] += 1\n",
    "            # False Negative\n",
    "            false_negatives[expected_tag] += 1\n",
    "\n",
    "    # calculando precisão, recall e f1-score para cada tag\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1_score = {}\n",
    "\n",
    "    for tag in set(true_positives.keys()).union(false_positives.keys()).union(false_negatives.keys()):\n",
    "        tp = true_positives[tag]\n",
    "        fp = false_positives[tag]\n",
    "        fn = false_negatives[tag]\n",
    "        \n",
    "        # Precisão: TP / (TP + FP)\n",
    "        precision[tag] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        \n",
    "        # recall: TP / (TP + FN)\n",
    "        recall[tag] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        # f1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        if precision[tag] + recall[tag] > 0:\n",
    "            f1_score[tag] = 2 * (precision[tag] * recall[tag]) / (precision[tag] + recall[tag])\n",
    "        else:\n",
    "            f1_score[tag] = 0\n",
    "\n",
    "    # print dos resultados em ordem decrescente de f1-score\n",
    "    for tag in sorted(f1_score.keys(), key=lambda t: f1_score[t], reverse=True):\n",
    "        print(f'TAG {tag} - Precision {precision[tag]:.2f} - Recall {recall[tag]:.2f} - F1 {f1_score[tag]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeiro modelo: Fine-tuning do modelo BioBERTpt(all) com corpus MacMorpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/francisco/Documents/Francisco/Faculdade/7_Semestre/processamento_lingua_natural/tp2/pos_tagging/env_py/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pucpr-br/postagger-bio-portuguese\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"pucpr-br/postagger-bio-portuguese\")\n",
    "\n",
    "nlp_token_class = pipeline('ner', model=model, tokenizer=tokenizer, grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrindo dataset -> como foi-se utlizado um modelo já treinado, não é necessário dividir em treino e teste (não faz diferença também qual será usado)\n",
    "with open('data/macmorpho-train.txt', 'r') as file:\n",
    "    # store file content in a list\n",
    "    lines = file.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jersei_N atinge_V média_N de_PREP Cr$_CUR 1,4_NUM milhão_N na_PREP+ART venda_N da_PREP+ART Pinhal_NPROP em_PREP São_NPROP Paulo_NPROP ._PU\\n', 'Programe_V sua_PROADJ viagem_N à_PREP+ART Exposição_NPROP Nacional_NPROP do_NPROP Zebu_NPROP ,_PU que_PRO-KS começa_V dia_N 25_N ._PU\\n', 'Safra_N recorde_ADJ e_KC disponibilidade_N de_PREP crédito_N ativam_V vendas_N de_PREP máquinas_N agrícolas_ADJ ._PU\\n', 'A_ART desertificação_N tornou_V crítica_ADJ a_ART produtividade_N de_PREP 52_NUM mil_NUM km²_N na_PREP+ART região_N ._PU\\n']\n"
     ]
    }
   ],
   "source": [
    "print(lines[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organizando os dados, separando as palavras das tags\n",
    "words = []\n",
    "tags = []\n",
    "for line in lines[0:int(len(lines)*0.7)]: # usando apenas 70% dos dados -> problemas de performance com o Kernel Jupiter reiniciando com muitos dados.\n",
    "    #  separando as palavras das tags\n",
    "    words_tags = line.split()\n",
    "    for word_tag in words_tags:\n",
    "        # separando a palavra da tag\n",
    "        word, tag = word_tag.split('_')\n",
    "        words.append(word)\n",
    "        tags.append(tag.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Salto', 'sete', 'O', 'grande']\n",
      "['N', 'ADJ', 'ART', 'ADJ']\n"
     ]
    }
   ],
   "source": [
    "print(words[0:4])\n",
    "print(tags[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_words = len(words)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 518670/518670 [2:17:52<00:00, 62.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "# realizando a inferência das tags\n",
    "for word, tag in tqdm(zip(words, tags), total=total_words, desc=\"Processing words\"):\n",
    "    prediction = nlp_token_class(word)\n",
    "    result = {\n",
    "        'word': word,\n",
    "        'tag': tag,\n",
    "        'prediction': prediction[0]['entity_group']\n",
    "    }\n",
    "    if prediction[0]['entity_group'] == tag:\n",
    "        result['correct'] = True\n",
    "    else:\n",
    "        result['correct'] = False\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# salvando resultados em um arquivo para análise posterior\n",
    "\n",
    "with open('results/results_postagger-bio-portuguese.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrino o arquivo gerado\n",
    "with open('results/results_postagger-bio-portuguese.json', 'r') as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia geral: 0.68\n"
     ]
    }
   ],
   "source": [
    "# calculando acurácia do modelo\n",
    "measure_and_print_general_accuracy(results, total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG: PREP+PROPESS - ACURÁCIA: 0.98 - OCORRÊNCIAS: 225\n",
      "TAG: N - ACURÁCIA: 0.96 - OCORRÊNCIAS: 114296\n",
      "TAG: KC - ACURÁCIA: 0.96 - OCORRÊNCIAS: 11980\n",
      "TAG: PU - ACURÁCIA: 0.95 - OCORRÊNCIAS: 73027\n",
      "TAG: V - ACURÁCIA: 0.92 - OCORRÊNCIAS: 54271\n",
      "TAG: ADV - ACURÁCIA: 0.92 - OCORRÊNCIAS: 11689\n",
      "TAG: PREP+ART - ACURÁCIA: 0.89 - OCORRÊNCIAS: 33970\n",
      "TAG: PREP+ADV - ACURÁCIA: 0.88 - OCORRÊNCIAS: 25\n",
      "TAG: PCP - ACURÁCIA: 0.79 - OCORRÊNCIAS: 11310\n",
      "TAG: PROSUB - ACURÁCIA: 0.59 - OCORRÊNCIAS: 2745\n",
      "TAG: PDEN - ACURÁCIA: 0.57 - OCORRÊNCIAS: 2933\n",
      "TAG: NPROP - ACURÁCIA: 0.50 - OCORRÊNCIAS: 50000\n",
      "TAG: PROPESS - ACURÁCIA: 0.50 - OCORRÊNCIAS: 4728\n",
      "TAG: PREP+PROSUB - ACURÁCIA: 0.45 - OCORRÊNCIAS: 317\n",
      "TAG: PROADJ - ACURÁCIA: 0.43 - OCORRÊNCIAS: 7102\n",
      "TAG: PREP+PROADJ - ACURÁCIA: 0.37 - OCORRÊNCIAS: 889\n",
      "TAG: ADJ - ACURÁCIA: 0.36 - OCORRÊNCIAS: 22321\n",
      "TAG: IN - ACURÁCIA: 0.25 - OCORRÊNCIAS: 75\n",
      "TAG: ART - ACURÁCIA: 0.22 - OCORRÊNCIAS: 39167\n",
      "TAG: PREP - ACURÁCIA: 0.22 - OCORRÊNCIAS: 52318\n",
      "TAG: KS - ACURÁCIA: 0.09 - OCORRÊNCIAS: 6440\n",
      "TAG: NUM - ACURÁCIA: 0.07 - OCORRÊNCIAS: 11059\n",
      "TAG: CUR - ACURÁCIA: 0.00 - OCORRÊNCIAS: 1887\n",
      "TAG: PRO-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 5330\n",
      "TAG: ADV-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 478\n",
      "TAG: PREP+PRO-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 88\n"
     ]
    }
   ],
   "source": [
    "# observando resultados mais detalhados\n",
    "\n",
    "# agora é calculado a acurácia para cada tag\n",
    "total_tags, accuracy_tags = measure_and_print_accuracy_and_len_by_tag(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG PREP+PROPESS - Precision 1.00 - Recall 0.98 - F1 0.99\n",
      "TAG KC - Precision 0.97 - Recall 0.96 - F1 0.97\n",
      "TAG PU - Precision 0.96 - Recall 0.95 - F1 0.95\n",
      "TAG V - Precision 0.98 - Recall 0.92 - F1 0.95\n",
      "TAG PREP+ART - Precision 0.94 - Recall 0.89 - F1 0.91\n",
      "TAG PCP - Precision 0.95 - Recall 0.79 - F1 0.86\n",
      "TAG PREP+ADV - Precision 0.76 - Recall 0.88 - F1 0.81\n",
      "TAG N - Precision 0.67 - Recall 0.96 - F1 0.79\n",
      "TAG ADV - Precision 0.63 - Recall 0.92 - F1 0.75\n",
      "TAG PDEN - Precision 0.92 - Recall 0.57 - F1 0.71\n",
      "TAG NPROP - Precision 0.91 - Recall 0.50 - F1 0.64\n",
      "TAG PROPESS - Precision 0.75 - Recall 0.50 - F1 0.60\n",
      "TAG PROADJ - Precision 0.90 - Recall 0.43 - F1 0.58\n",
      "TAG PREP+PROADJ - Precision 1.00 - Recall 0.37 - F1 0.54\n",
      "TAG ADJ - Precision 0.92 - Recall 0.36 - F1 0.52\n",
      "TAG ART - Precision 0.96 - Recall 0.22 - F1 0.36\n",
      "TAG PREP - Precision 0.94 - Recall 0.22 - F1 0.35\n",
      "TAG PREP+PROSUB - Precision 0.18 - Recall 0.45 - F1 0.26\n",
      "TAG IN - Precision 0.12 - Recall 0.25 - F1 0.16\n",
      "TAG PROSUB - Precision 0.08 - Recall 0.59 - F1 0.15\n",
      "TAG KS - Precision 0.28 - Recall 0.09 - F1 0.14\n",
      "TAG NUM - Precision 0.82 - Recall 0.07 - F1 0.13\n",
      "TAG ADV-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG CUR - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG <pad> - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG PREP+PRO-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG PRO-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n"
     ]
    }
   ],
   "source": [
    "# calculado precisão, recall e f1-score para cada tag\n",
    "measure_and_print_precision_recall_f1_by_tag(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo modelo: Fine-tuning do modelo BERTimbau com corpus MacMorpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# carregando o modelo\n",
    "pipe = pipeline(\"token-classification\", model=\"lisaterumi/postagger-portuguese\", tokenizer=\"lisaterumi/postagger-portuguese\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo dados de entrada novamente e separado melhor o arquivo\n",
    "with open('data/macmorpho-train.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "dataT = []\n",
    "\n",
    "for line in lines[0:int(len(lines)*0.7)]:\n",
    "    words_tags = line.split()\n",
    "    for word_tag in words_tags:\n",
    "        word, tag = word_tag.split('_')\n",
    "        dataT.append((word, tag.strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jersei', 'N'),\n",
       " ('atinge', 'V'),\n",
       " ('média', 'N'),\n",
       " ('de', 'PREP'),\n",
       " ('Cr$', 'CUR')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataT[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ',\n",
       " 'ADV',\n",
       " 'ADV-KS',\n",
       " 'ART',\n",
       " 'CUR',\n",
       " 'IN',\n",
       " 'KC',\n",
       " 'KS',\n",
       " 'N',\n",
       " 'NPROP',\n",
       " 'NUM',\n",
       " 'PCP',\n",
       " 'PDEN',\n",
       " 'PREP',\n",
       " 'PREP+ADV',\n",
       " 'PREP+ART',\n",
       " 'PREP+PRO-KS',\n",
       " 'PREP+PROADJ',\n",
       " 'PREP+PROPESS',\n",
       " 'PREP+PROSUB',\n",
       " 'PRO-KS',\n",
       " 'PROADJ',\n",
       " 'PROPESS',\n",
       " 'PROSUB',\n",
       " 'PU',\n",
       " 'V'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pegando todas as tags\n",
    "tags = set([tag for _, tag in dataT])\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words:   0%|          | 0/518670 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing words: 100%|██████████| 518670/518670 [2:20:09<00:00, 61.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# raelizando predição das tags\n",
    "results = []\n",
    "\n",
    "for word, expected_tag in tqdm(dataT, desc=\"Processing words\"):\n",
    "    prediction = pipe(word)\n",
    "    \n",
    "    # pegando a tag prevista\n",
    "    predicted_tag = prediction[0]['entity_group']\n",
    "    \n",
    "    result_data = {\n",
    "        'word': word,\n",
    "        'tag': expected_tag,\n",
    "        'prediction': predicted_tag,\n",
    "        'correct': predicted_tag == expected_tag,\n",
    "    }\n",
    "    \n",
    "    results.append(result_data)\n",
    "        \n",
    "# salvando resultados em um arquivo json\n",
    "with open('results/results_postagger-portuguese.json', 'w') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrindo arquivo para analise dos resultados -> acuracia geral\n",
    "\n",
    "with open('results/results_postagger-portuguese.json', 'r') as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia geral: 0.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_words = len(results)\n",
    "\n",
    "measure_and_print_general_accuracy(results, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG: PU - ACURÁCIA: 1.00 - OCORRÊNCIAS: 73027\n",
      "TAG: PREP+PROPESS - ACURÁCIA: 0.98 - OCORRÊNCIAS: 225\n",
      "TAG: N - ACURÁCIA: 0.97 - OCORRÊNCIAS: 114296\n",
      "TAG: KC - ACURÁCIA: 0.96 - OCORRÊNCIAS: 11980\n",
      "TAG: PREP+ADV - ACURÁCIA: 0.92 - OCORRÊNCIAS: 25\n",
      "TAG: V - ACURÁCIA: 0.92 - OCORRÊNCIAS: 54271\n",
      "TAG: ADV - ACURÁCIA: 0.91 - OCORRÊNCIAS: 11689\n",
      "TAG: PROSUB - ACURÁCIA: 0.86 - OCORRÊNCIAS: 2745\n",
      "TAG: PREP - ACURÁCIA: 0.85 - OCORRÊNCIAS: 52318\n",
      "TAG: PCP - ACURÁCIA: 0.82 - OCORRÊNCIAS: 11310\n",
      "TAG: PROPESS - ACURÁCIA: 0.80 - OCORRÊNCIAS: 4728\n",
      "TAG: PREP+ART - ACURÁCIA: 0.75 - OCORRÊNCIAS: 33970\n",
      "TAG: PDEN - ACURÁCIA: 0.55 - OCORRÊNCIAS: 2933\n",
      "TAG: NPROP - ACURÁCIA: 0.51 - OCORRÊNCIAS: 50000\n",
      "TAG: ADJ - ACURÁCIA: 0.50 - OCORRÊNCIAS: 22321\n",
      "TAG: PREP+PROADJ - ACURÁCIA: 0.47 - OCORRÊNCIAS: 889\n",
      "TAG: IN - ACURÁCIA: 0.40 - OCORRÊNCIAS: 75\n",
      "TAG: PREP+PROSUB - ACURÁCIA: 0.40 - OCORRÊNCIAS: 317\n",
      "TAG: NUM - ACURÁCIA: 0.12 - OCORRÊNCIAS: 11059\n",
      "TAG: ART - ACURÁCIA: 0.08 - OCORRÊNCIAS: 39167\n",
      "TAG: PROADJ - ACURÁCIA: 0.05 - OCORRÊNCIAS: 7102\n",
      "TAG: KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 6440\n",
      "TAG: CUR - ACURÁCIA: 0.00 - OCORRÊNCIAS: 1887\n",
      "TAG: PRO-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 5330\n",
      "TAG: ADV-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 478\n",
      "TAG: PREP+PRO-KS - ACURÁCIA: 0.00 - OCORRÊNCIAS: 88\n"
     ]
    }
   ],
   "source": [
    "total_tags, accuracy_tags = measure_and_print_accuracy_and_len_by_tag(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG PREP+PROPESS - Precision 1.00 - Recall 0.98 - F1 0.99\n",
      "TAG KC - Precision 0.95 - Recall 0.96 - F1 0.96\n",
      "TAG V - Precision 0.98 - Recall 0.92 - F1 0.95\n",
      "TAG PREP - Precision 0.94 - Recall 0.85 - F1 0.89\n",
      "TAG PCP - Precision 0.95 - Recall 0.82 - F1 0.88\n",
      "TAG PREP+ADV - Precision 0.77 - Recall 0.92 - F1 0.84\n",
      "TAG PREP+ART - Precision 0.93 - Recall 0.75 - F1 0.83\n",
      "TAG PU - Precision 0.70 - Recall 1.00 - F1 0.82\n",
      "TAG N - Precision 0.66 - Recall 0.97 - F1 0.79\n",
      "TAG ADV - Precision 0.67 - Recall 0.91 - F1 0.78\n",
      "TAG PROPESS - Precision 0.72 - Recall 0.80 - F1 0.76\n",
      "TAG PDEN - Precision 0.93 - Recall 0.55 - F1 0.69\n",
      "TAG NPROP - Precision 0.99 - Recall 0.51 - F1 0.68\n",
      "TAG ADJ - Precision 0.89 - Recall 0.50 - F1 0.64\n",
      "TAG PREP+PROADJ - Precision 0.96 - Recall 0.47 - F1 0.63\n",
      "TAG IN - Precision 0.33 - Recall 0.40 - F1 0.36\n",
      "TAG PREP+PROSUB - Precision 0.31 - Recall 0.40 - F1 0.35\n",
      "TAG NUM - Precision 0.90 - Recall 0.12 - F1 0.21\n",
      "TAG ART - Precision 0.94 - Recall 0.08 - F1 0.15\n",
      "TAG PROSUB - Precision 0.08 - Recall 0.86 - F1 0.14\n",
      "TAG PROADJ - Precision 0.88 - Recall 0.05 - F1 0.09\n",
      "TAG KS - Precision 0.97 - Recall 0.00 - F1 0.01\n",
      "TAG ADV-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG <pad> - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG CUR - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG PRO-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n",
      "TAG PREP+PRO-KS - Precision 0.00 - Recall 0.00 - F1 0.00\n"
     ]
    }
   ],
   "source": [
    "measure_and_print_precision_recall_f1_by_tag(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
